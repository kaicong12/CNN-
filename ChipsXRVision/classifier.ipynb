{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b3a96d475c5811b150a2a2ffa2766deab9324f5fcf40d4cc985877e75fdd33fb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset\\\\valid'\n",
    "f_path = 'dataset\\\\faulty'\n",
    "\n",
    "images = glob.glob(os.path.join(path, '*.jpg'))\n",
    "\n",
    "rotate_methods = [cv2.cv2.ROTATE_90_CLOCKWISE, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.cv2.ROTATE_180]\n",
    "rotate_name = ['c_90d.jpg', 'cc_90d.jpg', '180d.jpg']\n",
    "\n",
    "for index, method in enumerate(rotate_methods):\n",
    "\tfor image in images:\n",
    "\t\tn = image.split('\\\\')[-1].split('.')[0]\n",
    "\t\ttemp = cv2.imread(image)\n",
    "\t\ttemp = cv2.rotate(temp, method)\n",
    "\t\tcv2.imwrite(os.path.join(f_path, n + '_' + rotate_name[index]), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import numpy as np \n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(hp.Int(f'input-unit',32,256,32), 3, input_shape=xtrain.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "\n",
    "    for n in range(hp.Int('layer', 1, 4)):\n",
    "        model.add(Conv2D(32, 3, activation='relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss = 'sparse_categorical_crossentropy',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['faulty','valid']\n",
    "path = 'dataset'\n",
    "data = []\n",
    "IMG_SIZE = 100 \n",
    "\n",
    "for category in categories:\n",
    "    folder = os.path.join(path,category)\n",
    "    label = categories.index(category)\n",
    "    for img in os.listdir(folder):\n",
    "        img_path = os.path.join(folder,img)\n",
    "        img_array = cv2.imread(img_path)\n",
    "        img_array = cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "        data.append([img_array,label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2411"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [feature for feature, label in data]\n",
    "y_data = [label for feature, label in data]\n",
    "\n",
    "x = np.array(x_data)\n",
    "x = x.astype('float32')/255\n",
    "y = np.array(y_data, dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project my_dir\\Chips\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from my_dir\\Chips\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials = 10,\n",
    "    executions_per_trial = 1,\n",
    "    directory='my_dir',\n",
    "    project_name='Chips'\n",
    ")\n",
    "\n",
    "my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=2)]\n",
    "tuner.search(x, y, epochs = 5, batch_size=64, validation_split=0.2, callbacks= my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results summary\nResults in my_dir\\Chips\nShowing 10 best trials\nObjective(name='val_accuracy', direction='max')\nTrial summary\nHyperparameters:\ninput-unit: 96\nlayer: 3\nScore: 1.0\nTrial summary\nHyperparameters:\ninput-unit: 32\nlayer: 4\nScore: 1.0\nTrial summary\nHyperparameters:\ninput-unit: 32\nlayer: 3\nScore: 1.0\nTrial summary\nHyperparameters:\ninput-unit: 32\nlayer: 1\nScore: 1.0\nTrial summary\nHyperparameters:\ninput-unit: 128\nlayer: 3\nScore: 1.0\nTrial summary\nHyperparameters:\ninput-unit: 160\nlayer: 3\nScore: 0.9994813203811646\nTrial summary\nHyperparameters:\ninput-unit: 160\nlayer: 2\nScore: 0.9994813203811646\nTrial summary\nHyperparameters:\ninput-unit: 64\nlayer: 3\nScore: 0.9989626407623291\nTrial summary\nHyperparameters:\ninput-unit: 224\nlayer: 2\nScore: 0.9989626407623291\nTrial summary\nHyperparameters:\ninput-unit: 192\nlayer: 4\nScore: 0.9968879818916321\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 5.6929e-05 - accuracy: 1.0000\n",
      "[test less, test accuracy]: [0.00011970717605436221, 1.0]\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.save('best_model.h5')\n",
    "print('[test less, test accuracy]:', best_model.evaluate(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'faulty'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "test = cv2.imread('dataset/faulty/chips (38)_cc_90d.jpg')\n",
    "test = cv2.resize(test, (IMG_SIZE, IMG_SIZE))\n",
    "test = test.reshape((1, IMG_SIZE, IMG_SIZE, 3))\n",
    "predict = best_model.predict(test)\n",
    "class_name = categories[np.argmax(predict)]\n",
    "class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}